# **Speech Emotion Recognition using SVD with CNN, LSTM, and SVM**  

### ğŸ¯ **Overview**  
This project focuses on **Speech Emotion Recognition (SER)** using the **Toronto Emotional Speech Set (TESS) dataset**. It applies **Singular Value Decomposition (SVD)** for feature extraction and evaluates three classification models: **CNN, LSTM, and SVM**. The project is implemented in separate Jupyter notebooks for each model, providing insights into how different architectures handle emotional speech classification.  

---

## ğŸ“‚ **Project Structure**  

```
ğŸ“¦ Speech-Emotion-Recognition-SVD
â”‚-- ğŸ“œ README.md
â”‚-- ğŸ“œ requirements.txt  # Required dependencies
â”‚-- ğŸ“‚ dataset/  # Contains the TESS dataset (after preprocessing)
â”‚   â””-- tess_data/  
â”‚-- ğŸ“œ CNN.ipynb  # CNN-based implementation
â”‚-- ğŸ“œ LSTM.ipynb  # LSTM-based implementation
â”‚-- ğŸ“œ SVM.ipynb  # SVM-based implementation
```

---

## ğŸ™ï¸ **Dataset: TESS (Toronto Emotional Speech Set)**  
The **TESS dataset** contains recordings of **two female actors**, simulating emotions across **seven categories**:  

- **Angry**
- **Disgust**
- **Fear**
- **Happy**
- **Neutral**
- **Sad**
- **Surprised**  

Each model is trained on this dataset after applying **SVD for feature extraction**.

---

## ğŸ” **Models Implemented**  
This project compares the performance of three models for Speech Emotion Recognition:  

1ï¸âƒ£ **Convolutional Neural Network (CNN)** â€“ Extracts spatial and temporal patterns in speech signals.  
2ï¸âƒ£ **Long Short-Term Memory (LSTM)** â€“ Captures long-range dependencies in time-series speech data.  
3ï¸âƒ£ **Support Vector Machine (SVM)** â€“ A classical ML model for feature-based classification.  

Each model is implemented in its respective Jupyter notebook (`CNN.ipynb`, `LSTM.ipynb`, and `SVM.ipynb`).  

---

## ğŸ›  **Preprocessing & Feature Extraction**  
1. **Audio Preprocessing**  
   - Convert audio files to **Mel-Frequency Cepstral Coefficients (MFCCs)**.  
   - Normalize the extracted features for consistency.  

2. **Dimensionality Reduction using SVD**  
   - Singular Value Decomposition (SVD) is applied to reduce feature dimensionality while retaining essential information.  

3. **Training & Evaluation**  
   - Models are trained on processed features.  
   - **Accuracy, Precision, Recall, and F1-score** are used for performance evaluation.  

---

## ğŸ“Š **Performance Comparison**  
- The accuracy and loss curves for CNN and LSTM are visualized using **matplotlib**.  
- **Confusion matrices** are generated for all models to analyze misclassifications.  
- The impact of **SVD on model performance** is compared.  

---

## ğŸš€ **How to Run the Project**  

### 1ï¸âƒ£ **Clone the Repository**  
```bash
git clone https://github.com/your-username/Speech-Emotion-Recognition-SVD.git
cd Speech-Emotion-Recognition-SVD
```

### 2ï¸âƒ£ **Install Dependencies**  
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ **Run the Jupyter Notebook**  
```bash
jupyter notebook
```
- Open `CNN.ipynb`, `LSTM.ipynb`, or `SVM.ipynb` in Jupyter Notebook.  
- Run the cells to preprocess data, extract features, train models, and visualize results.  

---

## ğŸ“Œ **Technologies Used**  
- **Python**  
- **Librosa** (for audio processing)  
- **NumPy, Pandas**  
- **Scikit-Learn** (for SVM and evaluation metrics)  
- **TensorFlow/Keras** (for CNN & LSTM)  
- **Matplotlib, Seaborn** (for visualization)  

---

## ğŸ”® **Future Enhancements**  
- Implementing **transformer-based models** (like Wav2Vec, Whisper) for improved accuracy.  
- Expanding dataset coverage for multilingual speech recognition.  
- Experimenting with **Autoencoders** for unsupervised feature learning.  

---


## ğŸ“œ **License**  
This project is open-source and available under the **Apache License**.  

---
